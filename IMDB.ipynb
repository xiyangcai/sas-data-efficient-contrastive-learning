{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a93ee69-a38c-4791-bc1f-b7bb0e515014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79e9fa49-41c1-4fa8-b898-7f56ccef0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f03809-997c-4b22-9743-90c1d1893334",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3d8ed4-5df1-41a6-b35b-5727ac7dfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfad92c-c5b1-4e83-b8e1-2b94dc1d1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93ba698-5b96-4446-876f-2f88fc9bcde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7f324-2d08-4425-b9f9-d7d1cb5a0114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf13ed0d-664f-47f7-838c-d76c16f838b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(tokens, p=0.5):\n",
    "    if len(tokens) == 0:\n",
    "        return tokens\n",
    "\n",
    "    mask = np.random.rand(len(tokens)) > p\n",
    "    remaining_tokens = list(np.array(tokens)[mask])\n",
    "    # remaining_tokens = [token for token in tokens if random.uniform(0, 1) > p]\n",
    "    if len(remaining_tokens) == 0:\n",
    "        return [random.choice(tokens)]  # 如果全部删除，则随机保留一个\n",
    "    return remaining_tokens\n",
    "\n",
    "# aug = nas.AbstSummAug(model_path='t5-base', device='cuda')\n",
    "    \n",
    "def t5_sum(tokens, aug, n=1):\n",
    "    text = ' '.join(tokens)\n",
    "    augmented_text = aug.augment(text, n)\n",
    "    \n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc7ae0-c8b3-4f60-a47e-5e07cc20179b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "77ad4700-4ed2-4900-a206-4f72514b2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 20000\n",
      "Num Valid: 5000\n",
      "Num Test: 25000\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  include_lengths=True) # necessary for packed_padded_sequence\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
    "                                          split_ratio=0.8)\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Valid: {len(valid_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7666b49a-f75a-42cb-811b-f022ac9fb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('br')\n",
    "    text = ' '.join(words)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # stemmer = SnowballStemmer(\"english\")\n",
    "    words = [re.sub('\\W+', '', word) for word in words]\n",
    "    words = [word.lower().replace(' ', '') for word in words if word.lower() not in stop_words]\n",
    "    # stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    return words\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    new_data = []\n",
    "    for example in tqdm(dataset.examples, desc=\"Preprocessing text\"):\n",
    "        text = preprocess_text(example.text)\n",
    "        label = example.label\n",
    "        new_example = data.Example.fromlist(\n",
    "            [text, label], [('text', TEXT), ('label', LABEL)]\n",
    "        )\n",
    "        new_data.append(new_example)\n",
    "    dataset.examples = new_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "277188d1-9ca3-4669-9661-a784f742bab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fe534ca3c74f1e86d098f7390ba0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing text:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_train_data = preprocess_dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15403f69-f96b-42ab-a3a2-38417c6f0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_data[18].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "648193e1-2b0f-4aa4-a617-e3437bcef57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ppdb = naw.SynonymAug(aug_src=\"ppdb\", model_path='./nlpaug_model/ppdb-2.0-tldr', aug_p=0.5)\n",
    "aug_wordnet = naw.SynonymAug(aug_src=\"wordnet\", aug_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "059e808f-d870-4214-a8ad-24e4e3fc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_deletion(tokens, p=0.5):\n",
    "    if len(tokens) == 0:\n",
    "        return tokens\n",
    "\n",
    "    mask = np.random.rand(len(tokens)) > p\n",
    "    remaining_tokens = list(np.array(tokens)[mask])\n",
    "\n",
    "    if len(remaining_tokens) == 0:\n",
    "        return [random.choice(tokens)]\n",
    "\n",
    "    return remaining_tokens\n",
    "\n",
    "\n",
    "def synonym_wordnet_replacement(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    aug = naw.SynonymAug(aug_src=\"wordnet\", aug_p=0.5)\n",
    "    text = aug.augment(text)\n",
    "    words = word_tokenize(text[0])\n",
    "    return words\n",
    "\n",
    "\n",
    "def synonym_ppdb_replacement(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    aug = naw.SynonymAug(aug_src=\"ppdb\", model_path='./nlpaug_model/ppdb-2.0-tldr', aug_p=0.5)\n",
    "    text = aug.augment(text)\n",
    "    words = word_tokenize(text[0])\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6bcd38f5-e4bc-4a89-86e5-80defc332a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_window_selection(tokens):\n",
    "    text_len = len(tokens)\n",
    "    start, end = 0, 0\n",
    "    \n",
    "    while end - start < text_len / 2:\n",
    "        rang = np.random.choice(range(text_len), 2)\n",
    "        start, end = np.sort(rang)\n",
    "    return tokens[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "42ceeb7b-070f-43d7-bde6-15563e13e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: thought movie seemed like case study make movie part since filmmaker give 2 consistency problems remain beginning end plot extremely predictable using bits pieces previous successful war stories computer generated graphics much like viewing video game points seemed attempt director add realistic quality story interested budget get idea work find information seemed like project pushed limits low budget movie far resulting production drags viewer along story without imagination engaged actors nt bad plot needs innovation\n",
      "\n",
      "PPDB Augmented texts: thought movie seemed like case investigation undertake movie part since filmmaker give 2 consistency problems remain beginning end plot extremely predictable using patches pieces previous successful struggle stories computer generated graphics much like viewing video game points note attempt director add realistic quality story desirous budget get idea jobs find information seemed like project pushed limits low budget movie far amounting production drags viewer along story without imagination engaged helpers nt bad plot obligations innovation\n",
      "\n",
      "WordNet Augmented texts: thought movie seemed like case study make movie part since filmmaker give two consistency problems remain beginning closing plot extremely predictable using bits pieces previous successful war stories computer engender graphics much like viewing video game points seemed attempt director add realistic quality story interested budget get idea work find out information seemed like project pushed limits low budget movie far resulting production drag viewer along story without imaging engaged actors nt bad plot pauperism innovation\n",
      "\n",
      "Random deletion Augmented texts: thought movie case since filmmaker 2 consistency problems remain end plot extremely predictable using previous successful war stories computer generated much like viewing points seemed add realistic story interested idea work information seemed like project low budget movie production drags without engaged nt bad plot innovation\n",
      "\n",
      "Random window selection Augmented texts: bits pieces previous successful war stories computer generated graphics much like viewing video game points seemed attempt director add realistic quality story interested budget get idea work find information seemed like project pushed limits low budget movie far resulting production drags viewer along story without imagination engaged actors nt bad plot needs\n"
     ]
    }
   ],
   "source": [
    "# from data_proc.text_augmentation import synonym_ppdb_replacement\n",
    "\n",
    "example = clean_train_data[101].text\n",
    "\n",
    "print('Original texts:', ' '.join(example))\n",
    "print()\n",
    "\n",
    "aug_text = synonym_ppdb_replacement(example)\n",
    "print('PPDB Augmented texts:', ' '.join(aug_text))\n",
    "print()\n",
    "\n",
    "aug_text = synonym_wordnet_replacement(example)\n",
    "print('WordNet Augmented texts:', ' '.join(aug_text))\n",
    "print()\n",
    "\n",
    "aug_text = random_deletion(example)\n",
    "print('Random deletion Augmented texts:', ' '.join(aug_text))\n",
    "print()\n",
    "\n",
    "aug_text = random_window_selection(example)\n",
    "print('Random window selection Augmented texts:', ' '.join(aug_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b3f047-6440-4834-a9d7-d55983240f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: well watch film late one night simpl amaz great fantast script great act costum special effect plot twist wow fact see end come becom writer great would recommend film anyon especi like much terrif\n",
      "\n",
      "Augmented texts: well watch film previous one night simpl amaz great fantast script great act costum special effect secret plan twist wow fact see end come becom author great would recommend moving picture show anyon especi comparable much terrif\n"
     ]
    }
   ],
   "source": [
    "for example in clean_train_data:\n",
    "    text = example.text\n",
    "    text = ' '.join(text)\n",
    "    print('Original texts:', text)\n",
    "    print()\n",
    "    aug_text = aug.augment(text)\n",
    "    print('Augmented texts:', aug_text[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f2f04edf-1923-4fc2-b7b7-e98acf394268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefe2fa-1cf9-48d0-bd98-88136ac76086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d814d-f34f-4537-b87c-706afbacad96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2f067-d49a-4c95-94f0-834b4ec1c2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ee155-ad1d-433c-a718-5d3065caf83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2f05c39-cbfb-4fc5-ba27-99455670668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fdf00f2-9924-4408-96cb-90c7d0144d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset.dataset, max_size=20000, vectors=f\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33dbcb50-4c8e-4200-a8a6-03950fc1449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TextDatasetWrapper(trainset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "453bc13c-5a6d-4701-b38a-f924e95e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca12590a-ed1f-43d7-abcb-eb267478ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "for i in trainset:\n",
    "    print(i.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f78016-5223-45ab-97fd-6fab73184895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# 移除停用词和进行词干提取\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(words):\n",
    "    filtered_words = [word for word in words if (word.lower() not in stop_words) and (word not in string.punctuation)]\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210d9d52-2369-4fce-9916-9730f45c488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5829a6ba-d677-4b1c-8b57-5fee79125588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f2390a404f4b1d97c98a0e6b37a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aedde1381c04828b424070ff557635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_data = []\n",
    "for example in tqdm(train_data.examples, desc=\"Preprocess Trainset\"):\n",
    "    text = preprocess_text(example.text)\n",
    "    label = example.label\n",
    "    new_example = data.Example.fromlist([text, label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_train_data.append(new_example)\n",
    "train_data.examples = new_train_data\n",
    "\n",
    "new_test_data = []\n",
    "for example in tqdm(test_data.examples, desc=\"Preprocess Testset\"):\n",
    "    text = preprocess_text(example.text)\n",
    "    label = example.label\n",
    "    new_example = data.Example.fromlist([text, label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_test_data.append(new_example)\n",
    "test_data.examples = new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b55d25-6679-4f77-8a84-16b03947eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = []\n",
    "# aug = nas.AbstSummAug(model_path='t5-base', device='cuda')\n",
    "# aug = naw.SynonymAug(aug_src='ppdb', model_path='./nlpaug_model/ppdb-2.0-tldr')\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n",
    "for example in tqdm(train_data.examples):\n",
    "    # 对原始文本进行随机删除数据增强\n",
    "    # example_augmented_text = random_deletion(example.text)\n",
    "    example_augmented_text = t5_sum(example.text, aug, 1)\n",
    "    \n",
    "    # 创建一个新的例子并将其添加到训练数据中\n",
    "    new_example = data.Example.fromlist([example_augmented_text, example.label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_examples.append(new_example)\n",
    "train_data.examples.extend(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f33f24-fe79-45a0-b242-d14ade9b7d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcfce019-4d97-452b-bb4c-18b6d2641d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size=VOCABULARY_SIZE,\n",
    "                 # vectors='glove.6B.100d',\n",
    "                 vectors=vocab.GloVe(name='6B', dim=100),\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61cb574f-a368-49db-81a3-b5c73b518849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fac02c-f427-42f9-810c-9a4914d1bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([76, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "tensor([[ 498,    8,   50,  ...,   63, 3910,  121],\n",
      "        [  29,  314,  298,  ...,   24, 1093,   11],\n",
      "        [  49,    3,  422,  ...,   18,  869,  115],\n",
      "        ...,\n",
      "        [   7,  303,  853,  ...,   15,    8,  670],\n",
      "        [9142,  334,    3,  ...,   15,    3,    3],\n",
      "        [  32,    3,    4,  ...,    1,    1,    1]], device='cuda:0')\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([61, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([25, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    print(batch.text[0])\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20565704-d391-4f3e-934c-ec1f46ff1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        if output_dim is not None:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = None\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.cpu()).cuda()\n",
    "        \n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "\n",
    "        if self.fc is not None:\n",
    "            return self.fc(hidden.squeeze(0)).view(-1)\n",
    "        else:\n",
    "            return hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9226b12a-4b16-4a53-bb2c-ff1d23ca8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1826624c-6bd7-4555-98a5-b482423fbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.text\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += batch_data.label.size(0)\n",
    "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a27b029-8804-47da-9def-1008a7930948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/157 | Cost: 0.6926\n",
      "Epoch: 001/015 | Batch 050/157 | Cost: 0.6879\n",
      "Epoch: 001/015 | Batch 100/157 | Cost: 0.6870\n",
      "Epoch: 001/015 | Batch 150/157 | Cost: 0.5695\n",
      "training accuracy: 69.82%\n",
      "valid accuracy: 63.40%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 002/015 | Batch 000/157 | Cost: 0.6028\n",
      "Epoch: 002/015 | Batch 050/157 | Cost: 0.5619\n",
      "Epoch: 002/015 | Batch 100/157 | Cost: 0.6247\n",
      "Epoch: 002/015 | Batch 150/157 | Cost: 0.6169\n",
      "training accuracy: 75.76%\n",
      "valid accuracy: 58.18%\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 003/015 | Batch 000/157 | Cost: 0.5952\n",
      "Epoch: 003/015 | Batch 050/157 | Cost: 0.5368\n",
      "Epoch: 003/015 | Batch 100/157 | Cost: 0.4706\n",
      "Epoch: 003/015 | Batch 150/157 | Cost: 0.4776\n",
      "training accuracy: 78.40%\n",
      "valid accuracy: 61.46%\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 004/015 | Batch 000/157 | Cost: 0.4826\n",
      "Epoch: 004/015 | Batch 050/157 | Cost: 0.4834\n",
      "Epoch: 004/015 | Batch 100/157 | Cost: 0.4913\n",
      "Epoch: 004/015 | Batch 150/157 | Cost: 0.4758\n",
      "training accuracy: 81.89%\n",
      "valid accuracy: 58.74%\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 005/015 | Batch 000/157 | Cost: 0.4675\n",
      "Epoch: 005/015 | Batch 050/157 | Cost: 0.5491\n",
      "Epoch: 005/015 | Batch 100/157 | Cost: 0.4106\n",
      "Epoch: 005/015 | Batch 150/157 | Cost: 0.4057\n",
      "training accuracy: 82.53%\n",
      "valid accuracy: 58.40%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 006/015 | Batch 000/157 | Cost: 0.5251\n",
      "Epoch: 006/015 | Batch 050/157 | Cost: 0.4056\n",
      "Epoch: 006/015 | Batch 100/157 | Cost: 0.3849\n",
      "Epoch: 006/015 | Batch 150/157 | Cost: 0.3617\n",
      "training accuracy: 84.93%\n",
      "valid accuracy: 57.44%\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 007/015 | Batch 000/157 | Cost: 0.3172\n",
      "Epoch: 007/015 | Batch 050/157 | Cost: 0.4130\n",
      "Epoch: 007/015 | Batch 100/157 | Cost: 0.3391\n",
      "Epoch: 007/015 | Batch 150/157 | Cost: 0.3060\n",
      "training accuracy: 85.43%\n",
      "valid accuracy: 64.40%\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 008/015 | Batch 000/157 | Cost: 0.2617\n",
      "Epoch: 008/015 | Batch 050/157 | Cost: 0.2693\n",
      "Epoch: 008/015 | Batch 100/157 | Cost: 0.3117\n",
      "Epoch: 008/015 | Batch 150/157 | Cost: 0.3814\n",
      "training accuracy: 87.07%\n",
      "valid accuracy: 55.88%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 009/015 | Batch 000/157 | Cost: 0.2829\n",
      "Epoch: 009/015 | Batch 050/157 | Cost: 0.3878\n",
      "Epoch: 009/015 | Batch 100/157 | Cost: 0.3160\n",
      "Epoch: 009/015 | Batch 150/157 | Cost: 0.3700\n",
      "training accuracy: 86.83%\n",
      "valid accuracy: 53.50%\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 010/015 | Batch 000/157 | Cost: 0.3907\n",
      "Epoch: 010/015 | Batch 050/157 | Cost: 0.3282\n",
      "Epoch: 010/015 | Batch 100/157 | Cost: 0.4244\n",
      "Epoch: 010/015 | Batch 150/157 | Cost: 0.2985\n",
      "training accuracy: 89.23%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 011/015 | Batch 000/157 | Cost: 0.2685\n",
      "Epoch: 011/015 | Batch 050/157 | Cost: 0.2565\n",
      "Epoch: 011/015 | Batch 100/157 | Cost: 0.2763\n",
      "Epoch: 011/015 | Batch 150/157 | Cost: 0.2733\n",
      "training accuracy: 89.58%\n",
      "valid accuracy: 56.16%\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 012/015 | Batch 000/157 | Cost: 0.2891\n",
      "Epoch: 012/015 | Batch 050/157 | Cost: 0.3478\n",
      "Epoch: 012/015 | Batch 100/157 | Cost: 0.2438\n",
      "Epoch: 012/015 | Batch 150/157 | Cost: 0.2708\n",
      "training accuracy: 90.57%\n",
      "valid accuracy: 53.96%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 013/015 | Batch 000/157 | Cost: 0.2681\n",
      "Epoch: 013/015 | Batch 050/157 | Cost: 0.2616\n",
      "Epoch: 013/015 | Batch 100/157 | Cost: 0.3068\n",
      "Epoch: 013/015 | Batch 150/157 | Cost: 0.1834\n",
      "training accuracy: 91.46%\n",
      "valid accuracy: 54.60%\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 014/015 | Batch 000/157 | Cost: 0.1759\n",
      "Epoch: 014/015 | Batch 050/157 | Cost: 0.2556\n",
      "Epoch: 014/015 | Batch 100/157 | Cost: 0.3573\n",
      "Epoch: 014/015 | Batch 150/157 | Cost: 0.2015\n",
      "training accuracy: 91.65%\n",
      "valid accuracy: 57.64%\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 015/015 | Batch 000/157 | Cost: 0.2396\n",
      "Epoch: 015/015 | Batch 050/157 | Cost: 0.2784\n",
      "Epoch: 015/015 | Batch 100/157 | Cost: 0.2390\n",
      "Epoch: 015/015 | Batch 150/157 | Cost: 0.1527\n",
      "training accuracy: 92.01%\n",
      "valid accuracy: 58.26%\n",
      "Time elapsed: 0.85 min\n",
      "Total Training Time: 0.85 min\n",
      "Test accuracy: 85.74%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.text\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791822e8-1a9a-4ff7-9062-1df1deefb3f0",
   "metadata": {},
   "source": [
    "## No Aug\n",
    "\n",
    "training accuracy: 90.28%\n",
    "valid accuracy: 84.80%\n",
    "Time elapsed: 1.28 min\n",
    "Total Training Time: 1.28 min\n",
    "Test accuracy: 84.42%\n",
    "\n",
    "## Rand Del\n",
    "training accuracy: 90.51%\n",
    "valid accuracy: 85.56%\n",
    "Time elapsed: 1.99 min\n",
    "Total Training Time: 1.99 min\n",
    "Test accuracy: 85.02%\n",
    "\n",
    "## Synonym Aug\n",
    "training accuracy: 66.77%\n",
    "valid accuracy: 80.44%\n",
    "Time elapsed: 1.50 min\n",
    "Total Training Time: 1.50 min\n",
    "Test accuracy: 79.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281aa939-a1ee-463d-bce2-ef58b73fbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a188deb8-92dd-4442-b551-a3dbe4d6be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5735723376274109"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Probability positive:')\n",
    "predict_sentiment(model, \"Do i really love this movie? Yes I do\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa97060-8524-4d28-a974-b889b2a2db0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
