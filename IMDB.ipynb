{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a93ee69-a38c-4791-bc1f-b7bb0e515014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e9fa49-41c1-4fa8-b898-7f56ccef0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f03809-997c-4b22-9743-90c1d1893334",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3d8ed4-5df1-41a6-b35b-5727ac7dfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfad92c-c5b1-4e83-b8e1-2b94dc1d1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93ba698-5b96-4446-876f-2f88fc9bcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(aug_src=\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7f324-2d08-4425-b9f9-d7d1cb5a0114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf13ed0d-664f-47f7-838c-d76c16f838b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(tokens, p=0.5):\n",
    "    if len(tokens) == 0:\n",
    "        return tokens\n",
    "\n",
    "    mask = np.random.rand(len(tokens)) > p\n",
    "    remaining_tokens = list(np.array(tokens)[mask])\n",
    "    # remaining_tokens = [token for token in tokens if random.uniform(0, 1) > p]\n",
    "    if len(remaining_tokens) == 0:\n",
    "        return [random.choice(tokens)]  # 如果全部删除，则随机保留一个\n",
    "    return remaining_tokens\n",
    "\n",
    "# aug = nas.AbstSummAug(model_path='t5-base', device='cuda')\n",
    "    \n",
    "def t5_sum(tokens, aug, n=1):\n",
    "    text = ' '.join(tokens)\n",
    "    augmented_text = aug.augment(text, n)\n",
    "    \n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc7ae0-c8b3-4f60-a47e-5e07cc20179b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ad4700-4ed2-4900-a206-4f72514b2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 20000\n",
      "Num Valid: 5000\n",
      "Num Test: 25000\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  include_lengths=True) # necessary for packed_padded_sequence\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
    "                                          split_ratio=0.8)\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Valid: {len(valid_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fce7320-fd60-43e3-a16a-63a53c4ef120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "249ca7cf-0290-4dea-8e18-16657cf50266",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2657931-7081-4c1c-a04c-90ddeb031698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'i' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617b250-18fc-4e00-8f31-7e7eb0114a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9b3f047-6440-4834-a9d7-d55983240f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well when watching this film late one night I was simple baffle by information technology ' s immensity. Fantastic script, great acting, costume and special effects, and the plot twists, wow! ! In fact if you lavatory see the terminate coming you should become a writer yourself. <br /> <br /> Great, I would recommend this film to anyone, especially if I don; deoxythymidine monophosphate like them much. <br /> <br /> Terrific\"]\n",
      "['Well', 'when', 'watching', 'this', 'film', 'late', 'one', 'night', 'I', 'was', 'simple', 'baffle', 'by', 'information', 'technology', \"'\", 's', 'immensity', '.', 'Fantastic', 'script', ',', 'great', 'acting', ',', 'costume', 'and', 'special', 'effects', ',', 'and', 'the', 'plot', 'twists', ',', 'wow', '!', '!', 'In', 'fact', 'if', 'you', 'lavatory', 'see', 'the', 'terminate', 'coming', 'you', 'should', 'become', 'a', 'writer', 'yourself', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Great', ',', 'I', 'would', 'recommend', 'this', 'film', 'to', 'anyone', ',', 'especially', 'if', 'I', 'don', ';', 'deoxythymidine', 'monophosphate', 'like', 'them', 'much', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Terrific']\n"
     ]
    }
   ],
   "source": [
    "for example in train_data:\n",
    "    text = example.text\n",
    "    text = ' '.join(text)\n",
    "    # print(text)\n",
    "    aug_text = aug.augment(text)\n",
    "    print(aug_text)\n",
    "    text = word_tokenize(aug_text[0])\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f04edf-1923-4fc2-b7b7-e98acf394268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2f05c39-cbfb-4fc5-ba27-99455670668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fdf00f2-9924-4408-96cb-90c7d0144d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset.dataset, max_size=20000, vectors=f\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33dbcb50-4c8e-4200-a8a6-03950fc1449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TextDatasetWrapper(trainset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "453bc13c-5a6d-4701-b38a-f924e95e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca12590a-ed1f-43d7-abcb-eb267478ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "for i in trainset:\n",
    "    print(i.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f78016-5223-45ab-97fd-6fab73184895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# 移除停用词和进行词干提取\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(words):\n",
    "    filtered_words = [word for word in words if (word.lower() not in stop_words) and (word not in string.punctuation)]\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210d9d52-2369-4fce-9916-9730f45c488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5829a6ba-d677-4b1c-8b57-5fee79125588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f2390a404f4b1d97c98a0e6b37a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aedde1381c04828b424070ff557635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_data = []\n",
    "for example in tqdm(train_data.examples, desc=\"Preprocess Trainset\"):\n",
    "    text = preprocess_text(example.text)\n",
    "    label = example.label\n",
    "    new_example = data.Example.fromlist([text, label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_train_data.append(new_example)\n",
    "train_data.examples = new_train_data\n",
    "\n",
    "new_test_data = []\n",
    "for example in tqdm(test_data.examples, desc=\"Preprocess Testset\"):\n",
    "    text = preprocess_text(example.text)\n",
    "    label = example.label\n",
    "    new_example = data.Example.fromlist([text, label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_test_data.append(new_example)\n",
    "test_data.examples = new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b55d25-6679-4f77-8a84-16b03947eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = []\n",
    "# aug = nas.AbstSummAug(model_path='t5-base', device='cuda')\n",
    "# aug = naw.SynonymAug(aug_src='ppdb', model_path='./nlpaug_model/ppdb-2.0-tldr')\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n",
    "for example in tqdm(train_data.examples):\n",
    "    # 对原始文本进行随机删除数据增强\n",
    "    # example_augmented_text = random_deletion(example.text)\n",
    "    example_augmented_text = t5_sum(example.text, aug, 1)\n",
    "    \n",
    "    # 创建一个新的例子并将其添加到训练数据中\n",
    "    new_example = data.Example.fromlist([example_augmented_text, example.label], [('text', TEXT), ('label', LABEL)])\n",
    "    new_examples.append(new_example)\n",
    "train_data.examples.extend(new_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f33f24-fe79-45a0-b242-d14ade9b7d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcfce019-4d97-452b-bb4c-18b6d2641d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size=VOCABULARY_SIZE,\n",
    "                 # vectors='glove.6B.100d',\n",
    "                 vectors=vocab.GloVe(name='6B', dim=100),\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61cb574f-a368-49db-81a3-b5c73b518849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fac02c-f427-42f9-810c-9a4914d1bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([76, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "tensor([[ 498,    8,   50,  ...,   63, 3910,  121],\n",
      "        [  29,  314,  298,  ...,   24, 1093,   11],\n",
      "        [  49,    3,  422,  ...,   18,  869,  115],\n",
      "        ...,\n",
      "        [   7,  303,  853,  ...,   15,    8,  670],\n",
      "        [9142,  334,    3,  ...,   15,    3,    3],\n",
      "        [  32,    3,    4,  ...,    1,    1,    1]], device='cuda:0')\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([61, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([25, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    print(batch.text[0])\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.text[0].size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20565704-d391-4f3e-934c-ec1f46ff1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        if output_dim is not None:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        else:\n",
    "            self.fc = None\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.cpu()).cuda()\n",
    "        \n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "\n",
    "        if self.fc is not None:\n",
    "            return self.fc(hidden.squeeze(0)).view(-1)\n",
    "        else:\n",
    "            return hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9226b12a-4b16-4a53-bb2c-ff1d23ca8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1826624c-6bd7-4555-98a5-b482423fbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.text\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += batch_data.label.size(0)\n",
    "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a27b029-8804-47da-9def-1008a7930948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/157 | Cost: 0.6926\n",
      "Epoch: 001/015 | Batch 050/157 | Cost: 0.6879\n",
      "Epoch: 001/015 | Batch 100/157 | Cost: 0.6870\n",
      "Epoch: 001/015 | Batch 150/157 | Cost: 0.5695\n",
      "training accuracy: 69.82%\n",
      "valid accuracy: 63.40%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 002/015 | Batch 000/157 | Cost: 0.6028\n",
      "Epoch: 002/015 | Batch 050/157 | Cost: 0.5619\n",
      "Epoch: 002/015 | Batch 100/157 | Cost: 0.6247\n",
      "Epoch: 002/015 | Batch 150/157 | Cost: 0.6169\n",
      "training accuracy: 75.76%\n",
      "valid accuracy: 58.18%\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 003/015 | Batch 000/157 | Cost: 0.5952\n",
      "Epoch: 003/015 | Batch 050/157 | Cost: 0.5368\n",
      "Epoch: 003/015 | Batch 100/157 | Cost: 0.4706\n",
      "Epoch: 003/015 | Batch 150/157 | Cost: 0.4776\n",
      "training accuracy: 78.40%\n",
      "valid accuracy: 61.46%\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 004/015 | Batch 000/157 | Cost: 0.4826\n",
      "Epoch: 004/015 | Batch 050/157 | Cost: 0.4834\n",
      "Epoch: 004/015 | Batch 100/157 | Cost: 0.4913\n",
      "Epoch: 004/015 | Batch 150/157 | Cost: 0.4758\n",
      "training accuracy: 81.89%\n",
      "valid accuracy: 58.74%\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 005/015 | Batch 000/157 | Cost: 0.4675\n",
      "Epoch: 005/015 | Batch 050/157 | Cost: 0.5491\n",
      "Epoch: 005/015 | Batch 100/157 | Cost: 0.4106\n",
      "Epoch: 005/015 | Batch 150/157 | Cost: 0.4057\n",
      "training accuracy: 82.53%\n",
      "valid accuracy: 58.40%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 006/015 | Batch 000/157 | Cost: 0.5251\n",
      "Epoch: 006/015 | Batch 050/157 | Cost: 0.4056\n",
      "Epoch: 006/015 | Batch 100/157 | Cost: 0.3849\n",
      "Epoch: 006/015 | Batch 150/157 | Cost: 0.3617\n",
      "training accuracy: 84.93%\n",
      "valid accuracy: 57.44%\n",
      "Time elapsed: 0.34 min\n",
      "Epoch: 007/015 | Batch 000/157 | Cost: 0.3172\n",
      "Epoch: 007/015 | Batch 050/157 | Cost: 0.4130\n",
      "Epoch: 007/015 | Batch 100/157 | Cost: 0.3391\n",
      "Epoch: 007/015 | Batch 150/157 | Cost: 0.3060\n",
      "training accuracy: 85.43%\n",
      "valid accuracy: 64.40%\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 008/015 | Batch 000/157 | Cost: 0.2617\n",
      "Epoch: 008/015 | Batch 050/157 | Cost: 0.2693\n",
      "Epoch: 008/015 | Batch 100/157 | Cost: 0.3117\n",
      "Epoch: 008/015 | Batch 150/157 | Cost: 0.3814\n",
      "training accuracy: 87.07%\n",
      "valid accuracy: 55.88%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 009/015 | Batch 000/157 | Cost: 0.2829\n",
      "Epoch: 009/015 | Batch 050/157 | Cost: 0.3878\n",
      "Epoch: 009/015 | Batch 100/157 | Cost: 0.3160\n",
      "Epoch: 009/015 | Batch 150/157 | Cost: 0.3700\n",
      "training accuracy: 86.83%\n",
      "valid accuracy: 53.50%\n",
      "Time elapsed: 0.51 min\n",
      "Epoch: 010/015 | Batch 000/157 | Cost: 0.3907\n",
      "Epoch: 010/015 | Batch 050/157 | Cost: 0.3282\n",
      "Epoch: 010/015 | Batch 100/157 | Cost: 0.4244\n",
      "Epoch: 010/015 | Batch 150/157 | Cost: 0.2985\n",
      "training accuracy: 89.23%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 011/015 | Batch 000/157 | Cost: 0.2685\n",
      "Epoch: 011/015 | Batch 050/157 | Cost: 0.2565\n",
      "Epoch: 011/015 | Batch 100/157 | Cost: 0.2763\n",
      "Epoch: 011/015 | Batch 150/157 | Cost: 0.2733\n",
      "training accuracy: 89.58%\n",
      "valid accuracy: 56.16%\n",
      "Time elapsed: 0.62 min\n",
      "Epoch: 012/015 | Batch 000/157 | Cost: 0.2891\n",
      "Epoch: 012/015 | Batch 050/157 | Cost: 0.3478\n",
      "Epoch: 012/015 | Batch 100/157 | Cost: 0.2438\n",
      "Epoch: 012/015 | Batch 150/157 | Cost: 0.2708\n",
      "training accuracy: 90.57%\n",
      "valid accuracy: 53.96%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 013/015 | Batch 000/157 | Cost: 0.2681\n",
      "Epoch: 013/015 | Batch 050/157 | Cost: 0.2616\n",
      "Epoch: 013/015 | Batch 100/157 | Cost: 0.3068\n",
      "Epoch: 013/015 | Batch 150/157 | Cost: 0.1834\n",
      "training accuracy: 91.46%\n",
      "valid accuracy: 54.60%\n",
      "Time elapsed: 0.74 min\n",
      "Epoch: 014/015 | Batch 000/157 | Cost: 0.1759\n",
      "Epoch: 014/015 | Batch 050/157 | Cost: 0.2556\n",
      "Epoch: 014/015 | Batch 100/157 | Cost: 0.3573\n",
      "Epoch: 014/015 | Batch 150/157 | Cost: 0.2015\n",
      "training accuracy: 91.65%\n",
      "valid accuracy: 57.64%\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 015/015 | Batch 000/157 | Cost: 0.2396\n",
      "Epoch: 015/015 | Batch 050/157 | Cost: 0.2784\n",
      "Epoch: 015/015 | Batch 100/157 | Cost: 0.2390\n",
      "Epoch: 015/015 | Batch 150/157 | Cost: 0.1527\n",
      "training accuracy: 92.01%\n",
      "valid accuracy: 58.26%\n",
      "Time elapsed: 0.85 min\n",
      "Total Training Time: 0.85 min\n",
      "Test accuracy: 85.74%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.text\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791822e8-1a9a-4ff7-9062-1df1deefb3f0",
   "metadata": {},
   "source": [
    "## No Aug\n",
    "\n",
    "training accuracy: 90.28%\n",
    "valid accuracy: 84.80%\n",
    "Time elapsed: 1.28 min\n",
    "Total Training Time: 1.28 min\n",
    "Test accuracy: 84.42%\n",
    "\n",
    "## Rand Del\n",
    "training accuracy: 90.51%\n",
    "valid accuracy: 85.56%\n",
    "Time elapsed: 1.99 min\n",
    "Total Training Time: 1.99 min\n",
    "Test accuracy: 85.02%\n",
    "\n",
    "## Synonym Aug\n",
    "training accuracy: 66.77%\n",
    "valid accuracy: 80.44%\n",
    "Time elapsed: 1.50 min\n",
    "Total Training Time: 1.50 min\n",
    "Test accuracy: 79.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281aa939-a1ee-463d-bce2-ef58b73fbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a188deb8-92dd-4442-b551-a3dbe4d6be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5735723376274109"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Probability positive:')\n",
    "predict_sentiment(model, \"Do i really love this movie? Yes I do\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa97060-8524-4d28-a974-b889b2a2db0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
